{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import amazon_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m DEST_FOLDER \u001b[39m=\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mDATASET\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m#assert ALEXNET_IMAGE_FEATURES.exists()\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39massert\u001b[39;00m VIT_IMAGE_FEATURES\u001b[39m.\u001b[39mexists()\n\u001b[1;32m     16\u001b[0m \u001b[39massert\u001b[39;00m CLIP_IMAGE_FEATURES\u001b[39m.\u001b[39mexists()\n\u001b[1;32m     17\u001b[0m \u001b[39massert\u001b[39;00m BERT_TEXT_FEATURES\u001b[39m.\u001b[39mexists()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATASET = 'Movies_And_TV'\n",
    "#DATASET = 'Clothing_Shoes_and_Jewelry'\n",
    "\n",
    "VALIDATION_SIZE=0.15\n",
    "RANDOM_SEED = 20230219\n",
    "ALEXNET_IMAGE_FEATURES = Path(f'data/amazon/{DATASET}_alexnet_features.npz')\n",
    "VIT_IMAGE_FEATURES = Path(f'data/amazon/{DATASET}_vit_features.npz')\n",
    "CLIP_IMAGE_FEATURES = Path(f'data/amazon/{DATASET}_clipimage_features.npz')\n",
    "\n",
    "CLIP_TEXT_FEATURES = Path(f'data/amazon/{DATASET}_cliptext_features.npz')\n",
    "BERT_TEXT_FEATURES = Path(f'data/amazon/{DATASET}_bert_features.npz')\n",
    "DEST_FOLDER = Path(f'data/{DATASET}')\n",
    "\n",
    "#assert ALEXNET_IMAGE_FEATURES.exists()\n",
    "assert VIT_IMAGE_FEATURES.exists()\n",
    "assert CLIP_IMAGE_FEATURES.exists()\n",
    "assert BERT_TEXT_FEATURES.exists()\n",
    "assert CLIP_TEXT_FEATURES.exists()\n",
    "\n",
    "image_features = CLIP_IMAGE_FEATURES\n",
    "text_featues = CLIP_TEXT_FEATURES\n",
    "\n",
    "DEST_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "with open(DEST_FOLDER / 'signature.txt', mode='w') as f:\n",
    "    f.write(f'Image: {image_features.stem}\\nText: {text_featues.stem}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_ids(series: pd.Series) -> pd.Series:\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    unique_ids = series.unique()\n",
    "    return pd.Series(\n",
    "        index=rng.permutation(unique_ids), \n",
    "        data=range(len(unique_ids))\n",
    "    )\n",
    "\n",
    "def df_stats(df: pd.DataFrame) -> str:\n",
    "    n_items = len(df['asin'].unique())\n",
    "    n_users = len(df['reviewerID'].unique())\n",
    "    sparsity = 1. * len(df) / (n_users * n_items)\n",
    "    return f'{n_items} items {n_users} users. Sparsity {sparsity * 100:.3f}%'\n",
    "\n",
    "def save_numerized(\n",
    "    reviews: pd.DataFrame, \n",
    "    uids: pd.Series, \n",
    "    product_ids: pd.Series,\n",
    "    user_id_column: str,\n",
    "    product_ids_column: str,\n",
    "    columns: List[str],\n",
    "    dest: Path\n",
    "):\n",
    "    \"\"\"Save a Dataframe following userids and product_ids\"\"\"\n",
    "    joined = reviews.join(uids.to_frame(user_id_column), on='reviewerID')\n",
    "    assert joined[user_id_column].isna().sum() == 0\n",
    "    \n",
    "    joined = joined.join(product_ids.to_frame(product_ids_column), on='asin')\n",
    "    assert joined[product_ids_column].isna().sum() == 0\n",
    "    \n",
    "    res = joined[columns]\n",
    "    \n",
    "    res.to_csv(dest, index=False)\n",
    "\n",
    "    return res\n",
    "\n",
    "def split_train_test_proportion(df: pd.DataFrame, test_prop=0.2):\n",
    "    \"\"\"\n",
    "    Split the dataframe by reviewer and take exactly `test_prop` records\n",
    "    for test and leave the rest for training\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "    res = df.copy()\n",
    "    res['rnd'] = rng.random(size=len(df))\n",
    "    res['rnd_rank'] = res.groupby('reviewerID')['rnd'].rank(pct=True)\n",
    "    condition = res['rnd_rank'] <= test_prop\n",
    "    \n",
    "    (_, train), (_, test) = res.groupby(condition)\n",
    "    \n",
    "    assert(len(set(train.index) & set(test.index)) == 0)\n",
    "\n",
    "    return df.loc[train.index], df.loc[test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>overall</th>\n",
       "      <th>text</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177011</th>\n",
       "      <td>32088736</td>\n",
       "      <td>B01G44DB6A</td>\n",
       "      <td>ALM9NXNJR9IP0</td>\n",
       "      <td>V. Looney</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good quality</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>GOOD QUALITY</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129231</th>\n",
       "      <td>22120140</td>\n",
       "      <td>B01FBYBM8I</td>\n",
       "      <td>A2OW3BSI26H09G</td>\n",
       "      <td>Richard Abbott III</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love these new shades. They fit my face type...</td>\n",
       "      <td>2018-03-08</td>\n",
       "      <td>I love these new shades</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51616</th>\n",
       "      <td>11880885</td>\n",
       "      <td>B00GKOMC3S</td>\n",
       "      <td>A2TMZ5QICN6GJC</td>\n",
       "      <td>Tiffany</td>\n",
       "      <td>5.0</td>\n",
       "      <td>If these were not too small, I would have defi...</td>\n",
       "      <td>2018-03-08</td>\n",
       "      <td>I was very disappointed.</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id        asin      reviewerID        reviewerName  overall  \\\n",
       "177011  32088736  B01G44DB6A   ALM9NXNJR9IP0           V. Looney      4.0   \n",
       "129231  22120140  B01FBYBM8I  A2OW3BSI26H09G  Richard Abbott III      5.0   \n",
       "51616   11880885  B00GKOMC3S  A2TMZ5QICN6GJC             Tiffany      5.0   \n",
       "\n",
       "                                                     text  reviewTime  \\\n",
       "177011                                       Good quality  2018-09-19   \n",
       "129231  I love these new shades. They fit my face type...  2018-03-08   \n",
       "51616   If these were not too small, I would have defi...  2018-03-08   \n",
       "\n",
       "                         summary  verified  vote  \n",
       "177011              GOOD QUALITY      True   NaN  \n",
       "129231   I love these new shades      True   NaN  \n",
       "51616   I was very disappointed.      True   NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = amazon_dataset.reviews_df(DATASET)\n",
    "reviews_df.sample(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data for SEM-MacridVAE & DMRL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38493 items 23318 users. Sparsity 0.020%\n"
     ]
    }
   ],
   "source": [
    "print(df_stats(reviews_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort some users randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A30NKRF3KBGA06        0\n",
       "AL0XGCBE6Z22M         1\n",
       "AMT5LF0TKY67C         2\n",
       "A2BY8EVXA3NRHD        3\n",
       "AWE6KR1ELIYQ3         4\n",
       "                  ...  \n",
       "A1MFBF49ZFMH2N    23313\n",
       "A36AF5I7D0VO8F    23314\n",
       "A35ZS7JT3G9B8     23315\n",
       "A3GC94SEKQI3QU    23316\n",
       "A185C12Y9XLYGY    23317\n",
       "Length: 23318, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = generate_unique_ids(reviews_df['reviewerID'])\n",
    "user_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B000B6AV7K        0\n",
       "B0143D7EE4        1\n",
       "B0105V2DEY        2\n",
       "B014EY21H2        3\n",
       "B005LUROIK        4\n",
       "              ...  \n",
       "B00A9R2P7A    38488\n",
       "B017HK485S    38489\n",
       "B008H7UKYY    38490\n",
       "B006K6PJTK    38491\n",
       "B01DUSBHZ0    38492\n",
       "Length: 38493, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_ids = generate_unique_ids(reviews_df['asin'])\n",
    "item_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train test and validation by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, validation_test_reviews = split_train_test_proportion(reviews_df, test_prop=0.4)\n",
    "validation_reviews, test_reviews = split_train_test_proportion(validation_test_reviews, test_prop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113836"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(validation_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30728"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks. All users are in the train validation and test sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_reviews['reviewerID']) == set(test_reviews['reviewerID']) ==  set(validation_reviews['reviewerID'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the train set and the validation set all have at least one item (same check as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_size</th>\n",
       "      <th>test_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23318.000000</td>\n",
       "      <td>23318.000000</td>\n",
       "      <td>23318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.881894</td>\n",
       "      <td>1.474397</td>\n",
       "      <td>1.317780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.529635</td>\n",
       "      <td>1.185903</td>\n",
       "      <td>1.091225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_size      val_size     test_size\n",
       "count  23318.000000  23318.000000  23318.000000\n",
       "mean       4.881894      1.474397      1.317780\n",
       "std        3.529635      1.185903      1.091225\n",
       "min        3.000000      1.000000      1.000000\n",
       "25%        3.000000      1.000000      1.000000\n",
       "50%        4.000000      1.000000      1.000000\n",
       "75%        5.000000      2.000000      1.000000\n",
       "max       61.000000     20.000000     20.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    train_reviews.groupby('reviewerID').size().to_frame('train_size'),\n",
    "    validation_reviews.groupby('reviewerID').size().to_frame('val_size'),\n",
    "    test_reviews.groupby('reviewerID').size().to_frame('test_size')\n",
    "], axis=1).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids.to_csv(DEST_FOLDER / 'users.txt')\n",
    "item_ids.to_csv(DEST_FOLDER / 'items.txt', header=['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14891</td>\n",
       "      <td>30409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19073</td>\n",
       "      <td>29339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21394</td>\n",
       "      <td>38246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22275</td>\n",
       "      <td>38246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6565</td>\n",
       "      <td>38246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178934</th>\n",
       "      <td>5195</td>\n",
       "      <td>6878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178935</th>\n",
       "      <td>418</td>\n",
       "      <td>7919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178937</th>\n",
       "      <td>11611</td>\n",
       "      <td>5762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178938</th>\n",
       "      <td>16539</td>\n",
       "      <td>5762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178939</th>\n",
       "      <td>13738</td>\n",
       "      <td>3855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113836 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user   item\n",
       "0       14891  30409\n",
       "4       19073  29339\n",
       "6       21394  38246\n",
       "10      22275  38246\n",
       "14       6565  38246\n",
       "...       ...    ...\n",
       "178934   5195   6878\n",
       "178935    418   7919\n",
       "178937  11611   5762\n",
       "178938  16539   5762\n",
       "178939  13738   3855\n",
       "\n",
       "[113836 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMERIZED_COMMON_PARAMS = {\n",
    "    'uids': user_ids, \n",
    "    'product_ids': item_ids,\n",
    "    'user_id_column': 'user',\n",
    "    'product_ids_column': 'item',\n",
    "    'columns': ['user', 'item']\n",
    "}\n",
    "\n",
    "save_numerized(\n",
    "    train_reviews, \n",
    "    dest=DEST_FOLDER / 'train.txt',\n",
    "    **NUMERIZED_COMMON_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saved validation data'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_numerized(\n",
    "    validation_reviews, \n",
    "    dest=DEST_FOLDER / 'validation.txt',\n",
    "    **NUMERIZED_COMMON_PARAMS\n",
    ")\n",
    "'Saved validation data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saved test data'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_numerized(\n",
    "    test_reviews, \n",
    "    dest=DEST_FOLDER / 'test.txt',\n",
    "    **NUMERIZED_COMMON_PARAMS\n",
    ")\n",
    "'Saved test data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file\n",
      "Initializing array (38493, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying features: 100%|██████████| 38.5k/38.5k [00:54<00:00, 703items/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38493, 1024)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def copy_features(\n",
    "        item_ids: pd.Series, \n",
    "        features_file: Path\n",
    "):\n",
    "    print('Opening file')\n",
    "    with np.load(features_file) as features:\n",
    "        some_embedding = next(iter(features.values()))\n",
    "        embedding_shape, = some_embedding.shape\n",
    "        array_shape = (len(item_ids),  embedding_shape)\n",
    "        print(f'Initializing array {array_shape}')\n",
    "        res = np.full(array_shape,  fill_value=np.nan)\n",
    "\n",
    "        for asin, idx in tqdm(item_ids.items(), \n",
    "                total=len(item_ids), unit_scale=True, unit='items', \n",
    "                desc='Copying features'):\n",
    "            value = features.get(asin)\n",
    "            if value is None:\n",
    "                print(f'Item {asin} not found in features file')\n",
    "                res[idx, :] = 0    \n",
    "            else:\n",
    "                assert np.isnan(value).sum() == 0, \"Feature has NaN Values\"\n",
    "                res[idx, :] = value\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "image_features_array = copy_features(\n",
    "    item_ids=item_ids, \n",
    "    features_file=image_features\n",
    ")\n",
    "image_features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Clothing_Shoes_and_Jewelry/embed_image.npy: 300MiB\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EMBED_DEST = DEST_FOLDER / 'embed_image.npy'\n",
    "np.save(IMAGE_EMBED_DEST, image_features_array)\n",
    "\n",
    "print(f'{str(IMAGE_EMBED_DEST)}: {IMAGE_EMBED_DEST.stat().st_size // 2**20}MiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file\n",
      "Initializing array (38493, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying features: 100%|██████████| 38.5k/38.5k [00:51<00:00, 741items/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38493, 1024)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_array = copy_features(\n",
    "    item_ids=item_ids, \n",
    "    features_file=text_featues\n",
    ")\n",
    "text_features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Clothing_Shoes_and_Jewelry/embed_text.npy: 300MiB\n"
     ]
    }
   ],
   "source": [
    "TEXT_EMBED_DEST = DEST_FOLDER / 'embed_text.npy'\n",
    "np.save(TEXT_EMBED_DEST, text_features_array)\n",
    "\n",
    "print(f'{str(TEXT_EMBED_DEST)}: {TEXT_EMBED_DEST.stat().st_size // 2**20}MiB')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/amazon/Clothing_Shoes_and_Jewelry_product_images')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df = amazon_dataset.products_df(DATASET)\n",
    "products_images_dir = amazon_dataset.product_images_dir(DATASET)\n",
    "products_images_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is necessary only for generating the fancy graph. Puts images on an transparent background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def convert_image(src: str, dst: str):\n",
    "    min_white = np.array([235, 235, 235], np.uint8)\n",
    "    white = np.array([255, 255, 255], np.uint8)\n",
    "    img = cv2.imread(src)\n",
    "    \n",
    "    mask = 255 - cv2.inRange(img, min_white, white)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
    "    res = cv2.bitwise_and(img, img, mask=mask)\n",
    "    cv2.imwrite(dst, res)\n",
    "\n",
    "def save_images(products_df: pd.DataFrame, item_ids: pd.Series, dest: Path):\n",
    "    products_by_asin = products_df.set_index('asin')\n",
    "    joined  = item_ids.to_frame('item_id').join(\n",
    "        products_by_asin, validate='one_to_one'\n",
    "    )\n",
    "    dest.mkdir(exist_ok=True)\n",
    "    \n",
    "    no_images = 0\n",
    "    \n",
    "    progress = tqdm(\n",
    "        joined.itertuples(), \n",
    "        desc='Copying images', \n",
    "        total=len(joined), \n",
    "        unit_scale=True, unit='items'\n",
    "    )\n",
    "    for row in progress:\n",
    "        if not isinstance(row.image_slug, list):\n",
    "            print('Not a list')\n",
    "            break\n",
    "        if len(row.image_slug) == 0:\n",
    "            no_images += 1\n",
    "            progress.set_postfix_str(f'No images: {no_images}', refresh=False)\n",
    "            continue\n",
    "        slug = row.image_slug[0]\n",
    "        src = products_images_dir / f\"{slug}.jpg\"\n",
    "        assert src.exists()\n",
    "        dst = dest / f'{row.item_id}.png'\n",
    "        convert_image(str(src), str(dst))\n",
    "\n",
    "#save_images(products_df, item_ids,  DEST_FOLDER / 'images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_df = amazon_dataset.product_categories_df(DATASET)\n",
    "# products_df = amazon_dataset.products_df(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_id = generate_unique_ids(categories_df['name'])\n",
    "# categories_id.to_csv(DEST_FOLDER / 'categories.txt', header=['category_id'])\n",
    "# categories_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_categories_matrix(\n",
    "        categories_df: pd.DataFrame, \n",
    "        products_df: pd.DataFrame, \n",
    "        item_ids: pd.Series, \n",
    "        categories_id: pd.Series\n",
    "    ):\n",
    "    MIN_SUPPORT = 0.01\n",
    "    MAX_SUPPORT = 0.9\n",
    "    \n",
    "    # Categories only with not too many or too few items\n",
    "    categories_support = categories_df['name'].value_counts() \n",
    "\n",
    "    allowed_categories_names = categories_support[1:8].index\n",
    "\n",
    "    print(\"How many allowed categories\")\n",
    "    print(len(allowed_categories_names))\n",
    "    print(allowed_categories_names)\n",
    "    \n",
    "    filtered_categories = categories_df.loc[\n",
    "        categories_df['name'].isin(allowed_categories_names)\n",
    "    ]\n",
    "\n",
    "    products_df = products_df.join(\n",
    "        item_ids.to_frame('item_id'), on='asin', validate='1:1')\n",
    "\n",
    "    res = filtered_categories.set_index('product_id').join(\n",
    "        products_df, validate='m:1')\n",
    "    res = res.reset_index(drop=True)\n",
    "    \n",
    "    # Check all categories are in categories_id\n",
    "    res = res.join(categories_id.to_frame('category_id'), on='name')\n",
    "    assert res['category_id'].isna().sum() == 0\n",
    "\n",
    "    items_with_no_category = set(item_ids) - set(res['item_id'].unique())\n",
    "    print(f\"items with no category: {len(items_with_no_category)}\")\n",
    "    \n",
    "    return products_df.loc[\n",
    "        products_df['asin'].isin(item_ids[items_with_no_category].index)\n",
    "    ]\n",
    "\n",
    "    return res[['item_id', 'category_id']]\n",
    "\n",
    "# categories_matrix = build_categories_matrix(\n",
    "#     categories_df, products_df, item_ids, categories_id\n",
    "# )\n",
    "#categories_matrix\n",
    "#categories_matrix.to_csv(DEST_FOLDER / 'categorical.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c22d1444aa3f68a820bd00264b529de9aca8b813336c5d71bda95dadce90fbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
