{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import amazon_dataset\n",
    "import feature_extraction_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Musical_Instruments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/amazon/Musical_Instruments_product_images')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_path = amazon_dataset.product_images_dir(dataset)\n",
    "asset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>rank</th>\n",
       "      <th>price</th>\n",
       "      <th>image_slug</th>\n",
       "      <th>image_url</th>\n",
       "      <th>feature</th>\n",
       "      <th>category</th>\n",
       "      <th>tech_detail</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46650</th>\n",
       "      <td>B004BR20OM</td>\n",
       "      <td>&lt;div class=\"aplus\"&gt;\\nThe versatile Blue Microp...</td>\n",
       "      <td>Blue Microphones Spark Condenser Microphone, C...</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Musical Instruments</td>\n",
       "      <td>&gt;#55,152 in Musical Instruments (See Top 100 i...</td>\n",
       "      <td></td>\n",
       "      <td>[315pG0OLK9L]</td>\n",
       "      <td>[https://m.media-amazon.com/images/I/315pG0OLK...</td>\n",
       "      <td>[Features Blue Microphones premium condenser c...</td>\n",
       "      <td>[Musical Instruments, Microphones &amp; Accessorie...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>B0002F78OE</td>\n",
       "      <td>The Pintech TC Series cymbals offer you one of...</td>\n",
       "      <td>Pintech Percussion TC16 16&amp;quot; Trigger Cymbal</td>\n",
       "      <td>Pintech Percussion</td>\n",
       "      <td>Musical Instruments</td>\n",
       "      <td>&gt;#62,397 in Musical Instruments (See Top 100 i...</td>\n",
       "      <td>$7.99</td>\n",
       "      <td>[41ueZ-QC1-L]</td>\n",
       "      <td>[https://m.media-amazon.com/images/I/41ueZ-QC1...</td>\n",
       "      <td>[Quiet playing surface, Great tracking, Great ...</td>\n",
       "      <td>[Musical Instruments, Drums &amp; Percussion, Elec...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55638</th>\n",
       "      <td>B005SGF28G</td>\n",
       "      <td>The Gibraltar BGC Basic Grabber Clamp is a two...</td>\n",
       "      <td>Gibraltar BGC Basic Grabber Clamp</td>\n",
       "      <td>Gibraltar</td>\n",
       "      <td>Musical Instruments</td>\n",
       "      <td>&gt;#7,445 in Musical Instruments (See Top 100 in...</td>\n",
       "      <td>$11.95</td>\n",
       "      <td>[31qWLX-Xw9L]</td>\n",
       "      <td>[https://m.media-amazon.com/images/I/31qWLX-Xw...</td>\n",
       "      <td>[Two hole economical version of the SC-4425G w...</td>\n",
       "      <td>[Musical Instruments, Instrument Accessories, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin                                        description  \\\n",
       "id                                                                     \n",
       "46650  B004BR20OM  <div class=\"aplus\">\\nThe versatile Blue Microp...   \n",
       "4984   B0002F78OE  The Pintech TC Series cymbals offer you one of...   \n",
       "55638  B005SGF28G  The Gibraltar BGC Basic Grabber Clamp is a two...   \n",
       "\n",
       "                                                   title               brand  \\\n",
       "id                                                                             \n",
       "46650  Blue Microphones Spark Condenser Microphone, C...                Blue   \n",
       "4984     Pintech Percussion TC16 16&quot; Trigger Cymbal  Pintech Percussion   \n",
       "55638                  Gibraltar BGC Basic Grabber Clamp           Gibraltar   \n",
       "\n",
       "                  main_cat                                               rank  \\\n",
       "id                                                                              \n",
       "46650  Musical Instruments  >#55,152 in Musical Instruments (See Top 100 i...   \n",
       "4984   Musical Instruments  >#62,397 in Musical Instruments (See Top 100 i...   \n",
       "55638  Musical Instruments  >#7,445 in Musical Instruments (See Top 100 in...   \n",
       "\n",
       "        price     image_slug  \\\n",
       "id                             \n",
       "46650          [315pG0OLK9L]   \n",
       "4984    $7.99  [41ueZ-QC1-L]   \n",
       "55638  $11.95  [31qWLX-Xw9L]   \n",
       "\n",
       "                                               image_url  \\\n",
       "id                                                         \n",
       "46650  [https://m.media-amazon.com/images/I/315pG0OLK...   \n",
       "4984   [https://m.media-amazon.com/images/I/41ueZ-QC1...   \n",
       "55638  [https://m.media-amazon.com/images/I/31qWLX-Xw...   \n",
       "\n",
       "                                                 feature  \\\n",
       "id                                                         \n",
       "46650  [Features Blue Microphones premium condenser c...   \n",
       "4984   [Quiet playing surface, Great tracking, Great ...   \n",
       "55638  [Two hole economical version of the SC-4425G w...   \n",
       "\n",
       "                                                category tech_detail  \n",
       "id                                                                    \n",
       "46650  [Musical Instruments, Microphones & Accessorie...        None  \n",
       "4984   [Musical Instruments, Drums & Percussion, Elec...        None  \n",
       "55638  [Musical Instruments, Instrument Accessories, ...        None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df = amazon_dataset.products_df(dataset)\n",
    "products_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features  : 100%|██████████| 8.31k/8.31k [00:20<00:00, 413image/s]\n",
      "Building feature dict: 100%|██████████| 8.46k/8.46k [00:02<00:00, 3.42kproduct/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with no values: 74\n"
     ]
    }
   ],
   "source": [
    "alexnet_features = feature_extraction_image.extract_alexnet_features(\n",
    "    products_df, \n",
    "    asset_path=asset_path,\n",
    "    batch_size=16\n",
    ")\n",
    "np.savez_compressed(\n",
    "    amazon_dataset.BASE_DATA_FOLDER / f'{dataset}_alexnet_features.npz',\n",
    "    **alexnet_features\n",
    ")\n",
    "del alexnet_features\n",
    "for feature_file in amazon_dataset.product_images_dir(dataset).glob('*.alexnet'):\n",
    "    feature_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features  :   0%|          | 16.0/8.30k [00:06<52:04, 2.65image/s]   \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.00 GiB (GPU 0; 5.79 GiB total capacity; 1.57 GiB already allocated; 887.81 MiB free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vit_features \u001b[39m=\u001b[39m feature_extraction_image\u001b[39m.\u001b[39;49mextract_vit_features(\n\u001b[1;32m      2\u001b[0m     products_df, \n\u001b[1;32m      3\u001b[0m     asset_path\u001b[39m=\u001b[39;49masset_path,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m np\u001b[39m.\u001b[39msavez_compressed(\n\u001b[1;32m      7\u001b[0m     amazon_dataset\u001b[39m.\u001b[39mBASE_DATA_FOLDER \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m}\u001b[39;00m\u001b[39m_vit_features.npz\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvit_features\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mdel\u001b[39;00m vit_features\n",
      "File \u001b[0;32m~/src/VBPR/feature_extraction_image.py:383\u001b[0m, in \u001b[0;36mextract_vit_features\u001b[0;34m(products_df, asset_path, force, batch_size, transform, device)\u001b[0m\n\u001b[1;32m    380\u001b[0m feature_extractor \u001b[39m=\u001b[39m ViTFeatureExtractor()\n\u001b[1;32m    381\u001b[0m suffix \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvit\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 383\u001b[0m run_feature_extractor(\n\u001b[1;32m    384\u001b[0m     asset_path\u001b[39m=\u001b[39;49masset_path,\n\u001b[1;32m    385\u001b[0m     force\u001b[39m=\u001b[39;49mforce,\n\u001b[1;32m    386\u001b[0m     feature_extractor\u001b[39m=\u001b[39;49mfeature_extractor,\n\u001b[1;32m    387\u001b[0m     suffix_name\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m    388\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    389\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    390\u001b[0m     device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m build_feature_dict(\n\u001b[1;32m    394\u001b[0m     products_df\u001b[39m=\u001b[39mproducts_df,\n\u001b[1;32m    395\u001b[0m     asset_path\u001b[39m=\u001b[39masset_path,\n\u001b[1;32m    396\u001b[0m     suffix_name\u001b[39m=\u001b[39msuffix\n\u001b[1;32m    397\u001b[0m )\n",
      "File \u001b[0;32m~/src/VBPR/feature_extraction_image.py:316\u001b[0m, in \u001b[0;36mrun_feature_extractor\u001b[0;34m(asset_path, feature_extractor, suffix_name, force, batch_size, transform, device)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset), unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    313\u001b[0m           smoothing\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExtracting features  \u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m progress, \\\n\u001b[1;32m    314\u001b[0m         ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39mmin\u001b[39m(\u001b[39m8\u001b[39m, batch_size)) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m    315\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m--> 316\u001b[0m         result \u001b[39m=\u001b[39m feature_extractor(batch[\u001b[39m'\u001b[39;49m\u001b[39mimg\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    317\u001b[0m         combined_results \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(batch[\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m], result[\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    319\u001b[0m         \u001b[39mfor\u001b[39;00m img_path, features \u001b[39min\u001b[39;00m combined_results:\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/VBPR/feature_extraction_image.py:231\u001b[0m, in \u001b[0;36mViTFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    230\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms(x)\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/fx/graph_module.py:658\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_wrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrapped_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/fx/graph_module.py:277\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_call(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcls, obj)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[39massert\u001b[39;00m e\u001b[39m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.5:34\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m _assert_3 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_assert(eq_3, \u001b[39m'\u001b[39m\u001b[39mExpected (batch_size, seq_length, hidden_dim) got Proxy(getattr_4)\u001b[39m\u001b[39m'\u001b[39m);  eq_3 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m encoder_layers_encoder_layer_0_ln_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mencoder_layer_0\u001b[39m.\u001b[39mln_1(encoder_dropout)\n\u001b[0;32m---> 34\u001b[0m encoder_layers_encoder_layer_0_self_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mencoder_layer_0\u001b[39m.\u001b[39;49mself_attention(query \u001b[39m=\u001b[39;49m encoder_layers_encoder_layer_0_ln_1, key \u001b[39m=\u001b[39;49m encoder_layers_encoder_layer_0_ln_1, value \u001b[39m=\u001b[39;49m encoder_layers_encoder_layer_0_ln_1, need_weights \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m);  encoder_layers_encoder_layer_0_ln_1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m getitem_5 \u001b[39m=\u001b[39m encoder_layers_encoder_layer_0_self_attention[\u001b[39m0\u001b[39m];  encoder_layers_encoder_layer_0_self_attention \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     36\u001b[0m encoder_layers_encoder_layer_0_dropout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mencoder_layer_0\u001b[39m.\u001b[39mdropout(getitem_5);  getitem_5 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1125\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         why_not_fast_path \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mgrad is enabled and at least one of query or the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1123\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39minput/output projection weights or biases requires_grad\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1124\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m why_not_fast_path:\n\u001b[0;32m-> 1125\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_native_multi_head_attention(\n\u001b[1;32m   1126\u001b[0m             query,\n\u001b[1;32m   1127\u001b[0m             key,\n\u001b[1;32m   1128\u001b[0m             value,\n\u001b[1;32m   1129\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim,\n\u001b[1;32m   1130\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1131\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight,\n\u001b[1;32m   1132\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1133\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1135\u001b[0m             key_padding_mask \u001b[39mif\u001b[39;49;00m key_padding_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m attn_mask,\n\u001b[1;32m   1136\u001b[0m             need_weights,\n\u001b[1;32m   1137\u001b[0m             average_attn_weights,\n\u001b[1;32m   1138\u001b[0m             \u001b[39m1\u001b[39;49m \u001b[39mif\u001b[39;49;00m key_padding_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m \u001b[39mif\u001b[39;49;00m attn_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1140\u001b[0m any_nested \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mis_nested \u001b[39mor\u001b[39;00m key\u001b[39m.\u001b[39mis_nested \u001b[39mor\u001b[39;00m value\u001b[39m.\u001b[39mis_nested\n\u001b[1;32m   1141\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m any_nested, (\u001b[39m\"\u001b[39m\u001b[39mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m   1142\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe fast path was not hit because \u001b[39m\u001b[39m{\u001b[39;00mwhy_not_fast_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.00 GiB (GPU 0; 5.79 GiB total capacity; 1.57 GiB already allocated; 887.81 MiB free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "vit_features = feature_extraction_image.extract_vit_features(\n",
    "    products_df, \n",
    "    asset_path=asset_path,\n",
    "    batch_size=16\n",
    ")\n",
    "np.savez_compressed(\n",
    "    amazon_dataset.BASE_DATA_FOLDER / f'{dataset}_vit_features.npz',\n",
    "    **vit_features\n",
    ")\n",
    "del vit_features\n",
    "for feature_file in amazon_dataset.product_images_dir(dataset).glob('*.vit'):\n",
    "    feature_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building feature dict: 100%|██████████| 23.9k/23.9k [00:05<00:00, 4.49kproduct/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with no values: 607\n"
     ]
    }
   ],
   "source": [
    "clip_features = feature_extraction_image.extract_clip_features(products_df, asset_path=asset_path)\n",
    "np.savez_compressed(\n",
    "    amazon_dataset.BASE_DATA_FOLDER / f'{dataset}_clipimage_features.npz',\n",
    "    **clip_features\n",
    ")\n",
    "for feature_file in amazon_dataset.product_images_dir(dataset).glob('*.clip'):\n",
    "    feature_file.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c22d1444aa3f68a820bd00264b529de9aca8b813336c5d71bda95dadce90fbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
