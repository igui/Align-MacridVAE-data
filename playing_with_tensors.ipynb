{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/amazon/Clothing_Shoes_and_Jewelry_product_images')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_path = Path('data/amazon/Clothing_Shoes_and_Jewelry_product_images/')\n",
    "asset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing images...\n",
      "All images have extracted features :)\n"
     ]
    }
   ],
   "source": [
    "feature_extraction.extract_alexnet_features(asset_path=asset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   0%|          | 64.0/294k [02:58<228:09:58, 2.79s/image] "
     ]
    }
   ],
   "source": [
    "feature_extraction.extract_vit_features(asset_path=asset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/amazon/Clothing_Shoes_and_Jewelry_product_images/41SQW8O6FPL.jpg\n",
      "data/amazon/Clothing_Shoes_and_Jewelry_product_images/41SQW8O6FPL.alexnet\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "some_image = choice(list(asset_path.glob('*.jpg')))\n",
    "print(some_image)\n",
    "some_feature = asset_path / f'{some_image.stem}.alexnet'\n",
    "print(some_feature)\n",
    "assert some_feature.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9216])\n",
      "(9216,)\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(feature_extraction)\n",
    "\n",
    "features = feature_extraction.extract_features_one_image(\n",
    "    image_path=some_image,\n",
    "    feature_extractor=feature_extraction.AlexNetFeatureExtractor(),\n",
    "    device='cpu'\n",
    ")\n",
    "print(features.size())\n",
    "loaded_feature = feature_extraction.load_features_from_file(some_feature)\n",
    "print(loaded_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2988)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.from_numpy(loaded_feature) - features.to('cpu')).abs().argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9813)\n",
      "1.9812548\n"
     ]
    }
   ],
   "source": [
    "print(features[0][2988])\n",
    "print(loaded_feature[2988])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import feature_extraction\n",
    "reload(feature_extraction)\n",
    "\n",
    "transforms = feature_extraction.SquaredCentered(size=(400,), fill=255)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "transformed_images = [\n",
    "    (image_path, transforms(image.to(device=device)))\n",
    "    for (image_path, image) in some_images\n",
    "]\n",
    "\n",
    "batch = torch.stack(\n",
    "    [image for (image_path, image) in transformed_images]\n",
    ").to(device)\n",
    "\n",
    "show(transformed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['n03775071', 'mitten'],\n",
       " ['n04336792', 'stretcher'],\n",
       " ['n02129604', 'tiger'],\n",
       " ['n02268853', 'damselfly'],\n",
       " ['n01749939', 'green_mamba'],\n",
       " ['n03873416', 'paddle'],\n",
       " ['n03459775', 'grille'],\n",
       " ['n11939491', 'daisy'],\n",
       " ['n02948072', 'candle'],\n",
       " ['n03450230', 'gown']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(Path('assets') / 'imagenet_class_index.json') as labels_file:\n",
    "    labels = json.load(labels_file)\n",
    "\n",
    "choices(list(labels.values()), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_feature_extractor = feature_extraction.AlexNetFeatureExtractor().to(device)\n",
    "vit_feature_extractor = feature_extraction.ViTFeatureExtractor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([836, 608, 445, 414, 770, 111, 885, 638, 650, 679, 672, 841, 459, 679,\n",
      "        770, 797, 600, 859, 578, 774, 887, 464, 906, 774, 485, 775, 770, 618,\n",
      "        499, 636, 770, 885, 841, 485, 841, 615, 667, 774, 578, 748, 512, 770,\n",
      "        775, 461, 502, 488, 488, 228, 885, 918, 578, 636, 885, 678, 623, 515,\n",
      "        610, 414, 414, 464, 614, 885, 906, 610, 610, 770, 522, 806, 608, 477,\n",
      "        842, 770, 608, 838, 402, 916, 916, 514, 514, 399, 608, 416, 893, 770,\n",
      "        679, 842, 655, 502, 770, 842, 416, 777, 608, 887, 697, 709, 836, 775,\n",
      "        601, 638, 459, 770, 893, 559, 813, 700, 592, 841, 549, 514, 601, 584,\n",
      "        643, 770, 885, 601, 610, 514, 399, 721, 869, 836, 398, 608, 761, 700,\n",
      "        770, 770, 697, 893,  21, 869, 691, 655, 752, 691, 414, 770, 502, 869,\n",
      "        911, 638, 721, 837, 749, 416, 841, 770, 639, 754, 748, 859, 793, 584,\n",
      "        514, 487, 842, 630, 770, 793, 597, 601, 883, 416, 770, 414, 861, 893,\n",
      "        643, 502, 545, 869, 615, 464, 459, 400, 608, 543, 689, 433, 610, 459,\n",
      "        639, 610, 778, 754, 584, 610, 748, 922, 939, 464, 465, 608, 446, 414,\n",
      "        655, 775, 600, 638, 639, 610, 887, 615, 546, 774, 414, 879, 655, 797,\n",
      "        823, 834, 885, 608, 797, 841, 806, 869, 414, 793, 482, 416, 906, 777,\n",
      "        922, 711, 506, 610, 578, 770, 502, 770, 638, 638, 524, 770, 502, 400,\n",
      "        416, 697, 563, 655, 774, 775, 584, 837, 514, 639, 783, 610, 459, 649,\n",
      "        610, 630, 885, 885, 608, 514, 893, 630, 770, 581, 797, 502, 916, 774,\n",
      "        841, 774, 836, 902, 885, 841, 549, 883, 837, 776, 608, 842, 770, 885,\n",
      "        887, 549, 911, 411, 610, 610, 824, 775, 601, 826, 794, 883, 578, 638,\n",
      "        811, 777, 869, 636, 465, 679, 499, 662, 902, 837, 854, 411, 414, 770,\n",
      "        841, 770, 770, 697, 465, 502, 770, 836, 459, 774, 608, 524, 427, 657,\n",
      "        445, 608, 610, 725, 885, 502, 922, 610, 655, 828, 502, 632, 636, 885,\n",
      "        679, 824, 885, 770, 770, 650, 841, 770, 615, 578, 775, 842, 639, 749,\n",
      "        414, 841, 638, 774, 639, 443, 770, 770, 615, 502, 697, 559, 869, 512,\n",
      "        409, 796, 485, 464, 503, 679, 679, 414, 842, 400, 459, 885, 774, 439,\n",
      "        499, 774, 633, 770, 885, 824, 411, 770, 911, 578, 490, 617, 610, 443,\n",
      "        797, 765, 610, 774, 496, 638, 916, 414, 502, 772, 697, 584, 502, 806,\n",
      "        630, 918, 679, 610, 608, 893, 502, 610, 834, 774, 869, 740, 593, 828,\n",
      "        679, 841, 503, 636, 357, 676, 775, 610, 636, 697, 796, 610, 774, 502,\n",
      "        887, 630, 411, 502, 608, 774, 411, 597, 129, 584, 411, 697, 610, 420,\n",
      "        400, 906, 770, 770, 824, 434, 710, 793, 514, 770, 655, 578, 911, 885,\n",
      "        770, 635, 640, 797, 834, 630, 679, 568, 602, 617, 126, 601, 885, 841,\n",
      "        459, 610, 600, 522, 655, 610, 774, 514, 775, 514, 770, 399, 600, 591,\n",
      "        459, 885, 606, 445, 610, 747, 551, 655, 770, 601, 502, 464, 777, 902,\n",
      "        414, 636, 638, 617, 770, 638, 655, 608, 798, 638, 639, 584, 638, 409,\n",
      "        808, 615, 655, 841, 636, 618, 790, 770, 399, 608, 691, 584, 841, 885,\n",
      "        697, 641, 885, 608, 828, 457, 824, 770, 824, 764, 610, 578, 916, 885,\n",
      "        770, 533, 488, 883, 679, 414, 601, 451, 399, 619, 411, 774, 841, 770,\n",
      "        885, 608, 414, 842, 844, 655, 608, 834, 893, 399, 551, 918, 639, 885,\n",
      "        735, 496, 836, 610, 610, 775, 499, 636, 885, 911, 639, 846, 556, 902,\n",
      "        750, 697, 443, 749, 916, 414, 806, 775, 697, 501, 893, 610, 893, 883,\n",
      "        610, 615, 473, 885, 655, 772, 869, 770, 514, 770, 414, 638, 630, 414,\n",
      "        423, 893, 630, 749, 774, 446, 655, 885, 841, 906, 597, 632, 828, 982,\n",
      "        615, 885, 623, 911, 474, 885, 593, 966, 842, 774, 502, 893, 679, 770,\n",
      "        869, 941, 636, 774, 414, 885, 419, 638, 699, 679, 515, 502, 885, 887,\n",
      "        502, 918, 477, 797, 641, 842, 774, 842, 902, 748, 578, 770, 512, 584,\n",
      "        723, 797, 414, 909, 770, 465, 841, 411, 578, 911, 828, 614, 457, 636,\n",
      "        774, 488, 399, 399, 774, 464, 414, 885, 411, 697, 770, 836, 556, 770,\n",
      "        869, 824, 922, 414, 600, 578, 411, 919, 770, 797, 568, 906, 631, 885,\n",
      "        578, 527, 502, 747, 630, 796, 824, 806, 608, 770, 885, 549, 837, 439,\n",
      "        770, 906, 841, 328, 531, 772, 680, 543, 770, 584, 796, 885, 911, 893,\n",
      "        770, 893, 828, 885, 708, 770, 584, 638, 892, 584, 893, 855, 597, 639,\n",
      "        735, 893, 459, 885, 608, 775, 735, 610, 893, 630, 474, 885, 885, 638,\n",
      "        770, 615, 842, 608, 911, 834, 610, 496, 499, 638, 459, 885, 399, 893,\n",
      "        692, 721, 832, 797, 700, 841, 770, 608, 679, 584, 615, 464, 593, 655,\n",
      "        635, 770, 678, 630, 770, 492,  45, 578, 727, 709, 515, 639, 601, 636,\n",
      "        499, 916, 630, 581, 797, 899, 770, 464, 608, 614, 399, 917, 714, 551,\n",
      "        610, 885, 633, 411, 608, 721, 796, 650, 748, 502, 770, 893, 701, 770,\n",
      "        638, 841, 869, 638, 650, 502, 770, 630, 639, 459, 774, 775, 464, 439,\n",
      "        774, 903, 459, 411, 608, 610, 630, 639, 769, 774, 601, 824, 615, 774,\n",
      "        796, 610, 610, 697, 543, 797, 726, 474, 869, 770, 893, 777, 885, 639,\n",
      "        502, 488, 893, 401, 465, 655, 697, 770, 414, 721, 614, 836, 639, 774,\n",
      "        608, 502, 824, 918, 868, 608, 626, 399, 549, 748, 464, 433, 828, 615,\n",
      "        502, 735, 774, 885, 885, 655, 797, 770, 638, 632, 551, 748, 419, 450,\n",
      "        608, 834, 916,  63, 638, 861, 774, 919, 610, 772, 869, 464, 461, 844,\n",
      "        735, 608, 533, 879, 617, 459, 836, 689, 679, 836, 502, 680, 502, 742,\n",
      "        841, 916, 414, 806, 608, 584, 862, 610, 578, 818, 823, 615, 584, 842,\n",
      "        414, 411, 916, 515, 824, 615, 697, 910, 893, 446, 502, 456, 414, 414,\n",
      "        836, 834, 869, 433, 401, 514, 775, 515, 830, 610, 770, 414, 697, 770,\n",
      "        898, 496, 697, 893, 400, 837], device='cuda:0')\n",
      "torch.Size([1000, 9216])\n"
     ]
    }
   ],
   "source": [
    "alexnet_features = alexnet_feature_extractor(batch)\n",
    "print(alexnet_features['output'].argmax(dim=1))\n",
    "print(alexnet_features['features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Image 1: ['n04120489', 'running_shoe']\n",
      "Prediction for Image 2: ['n03594734', 'jean']\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(alexnet_features['output'].argmax(dim=1), start=1):\n",
    "    print(f\"Prediction for Image {i}: {labels[str(pred.item())]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.93 GiB (GPU 0; 5.79 GiB total capacity; 4.09 GiB already allocated; 752.44 MiB free; 4.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vit_features \u001b[39m=\u001b[39m vit_feature_extractor(batch)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(vit_features[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(vit_features[\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/VBPR/feature_extraction.py:71\u001b[0m, in \u001b[0;36mViTFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     70\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 71\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(x)\n\u001b[1;32m     72\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody(x)\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torchvision/transforms/_presets.py:56\u001b[0m, in \u001b[0;36mImageClassification.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize_size, interpolation\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation)\n\u001b[1;32m     57\u001b[0m     img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcenter_crop(img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrop_size)\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, Tensor):\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    473\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    474\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 476\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49minterpolation\u001b[39m.\u001b[39;49mvalue, antialias\u001b[39m=\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:469\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    467\u001b[0m align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m interpolation \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m img \u001b[39m=\u001b[39m interpolate(img, size\u001b[39m=\u001b[39;49msize, mode\u001b[39m=\u001b[39;49minterpolation, align_corners\u001b[39m=\u001b[39;49malign_corners, antialias\u001b[39m=\u001b[39;49mantialias)\n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m interpolation \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m out_dtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39muint8:\n\u001b[1;32m    472\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m255\u001b[39m)\n",
      "File \u001b[0;32m~/src/VBPR/.venv/lib/python3.8/site-packages/torch/nn/functional.py:3958\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3956\u001b[0m     \u001b[39mif\u001b[39;00m antialias:\n\u001b[1;32m   3957\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_upsample_bicubic2d_aa(\u001b[39minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 3958\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_bicubic2d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3960\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3961\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGot 3D input, but bilinear mode needs 4D input\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.93 GiB (GPU 0; 5.79 GiB total capacity; 4.09 GiB already allocated; 752.44 MiB free; 4.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "vit_features = vit_feature_extractor(batch)\n",
    "print(vit_features['output'].argmax(dim=1))\n",
    "print(vit_features['features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Image 1: ['n04120489', 'running_shoe']\n",
      "Prediction for Image 2: ['n03594734', 'jean']\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(vit_features['output'].argmax(dim=1), start=1):\n",
    "    print(f\"Prediction for Image {i}: {labels[str(pred.item())]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c22d1444aa3f68a820bd00264b529de9aca8b813336c5d71bda95dadce90fbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
